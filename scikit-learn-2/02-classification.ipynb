{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "duration": "1.5 hours"
   },
   "source": [
    "# Machine Learning: Introduction to Classification\n",
    "\n",
    "Prepared by John C.S. Lui (www.cse.cuhk.edu.hk/~cslui) for the course CSCI3320 (Fundamentals of Machine Learning).\n",
    "\n",
    "#### Date:  April 6, 2021\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris dataset\n",
    "\n",
    "The dataset is a collection of measurements of several Iris flowers. These measurements will enable us to distinguish different species of the flowers. Features of inputs iclude:\n",
    "* sepal length\n",
    "* sepal width\n",
    "* petal length\n",
    "* petal width\n",
    "\n",
    "For each input, we also have the **label** to identify the type of Iris.  So we are considering **supervised learning**.  For this dataaset, the labels are:\n",
    "* Setosa \n",
    "* Versicolor\n",
    "* Virginica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use visualization to get a feeling on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# scikit-learn has several data sets, one of them is the iris data set.\n",
    "# We load the data with load_iris from scikit-learn\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()  # load_iris returns an object with several fields\n",
    "\n",
    "\n",
    "# features is a list, with each item being a list of 4 features of inputs\n",
    "features = data.data  \n",
    "feature_names = data.feature_names\n",
    "\n",
    "# target is a list, with each item being a class label of 0, 1, or 2 of inputs\n",
    "target = data.target  \n",
    "target_names = data.target_names\n",
    "\n",
    "# Let's examine our data\n",
    "print('first 10 features are: ', features[:10],'\\n')\n",
    "print('shape o features: ', features.shape, '\\n')\n",
    "print('first 10 target are: ', target[:10],'\\n')\n",
    "print('feature_names: ', feature_names,'\\n')\n",
    "print('target_names: ', target_names,'\\n')\n",
    "\n",
    "# Let's do a scatter sub-plot of feature 0 vs. feature 1\n",
    "\n",
    "plt.figure(num=None, figsize=(4,4))\n",
    "plt.clf()\n",
    "for t in range(3):\n",
    "  if t == 0: \n",
    "       c = 'r'\n",
    "       marker = '>' \n",
    "  elif t == 1:\n",
    "       c = 'g'\n",
    "       marker = 'o' \n",
    "  elif t == 2:\n",
    "       c = 'b'\n",
    "       marker = 'x' \n",
    "  plt.scatter(features[target == t,0], features[target == t,1], marker=marker, c=c)\n",
    "  plt.title('Sub-plot for 2 features')\n",
    "  plt.xlabel(feature_names[0])\n",
    "  plt.ylabel(feature_names[1])\n",
    "  \n",
    "  \n",
    "# Let's do a scatter sub-plot of feature 0 vs. feature 2\n",
    "\n",
    "plt.figure(num=None, figsize=(4,4))\n",
    "plt.clf()\n",
    "for t in range(3):\n",
    "  if t == 0: \n",
    "       c = 'r'\n",
    "       marker = '>' \n",
    "  elif t == 1:\n",
    "       c = 'g'\n",
    "       marker = 'o' \n",
    "  elif t == 2:\n",
    "       c = 'b'\n",
    "       marker = 'x' \n",
    "\n",
    "  # notice how we do select features via features[target==t, 0] or features[target==t, 2]\n",
    "  plt.scatter(features[target == t,0], features[target == t,2], marker=marker, c=c)\n",
    "  plt.title('Sub-plot for 2 features')\n",
    "  plt.xlabel(feature_names[0])\n",
    "  plt.ylabel(feature_names[2])\n",
    "    \n",
    "\n",
    "# Let's do a scatter sub-plot of feature 0 vs. feature 3\n",
    "\n",
    "plt.figure(num=None, figsize=(4,4))\n",
    "plt.clf()\n",
    "for t in range(3):\n",
    "  if t == 0: \n",
    "       c = 'r'\n",
    "       marker = '>' \n",
    "  elif t == 1:\n",
    "       c = 'g'\n",
    "       marker = 'o' \n",
    "  elif t == 2:\n",
    "       c = 'b'\n",
    "       marker = 'x' \n",
    "    \n",
    "  plt.scatter(features[target == t,0], features[target == t,3], marker=marker, c=c)\n",
    "  plt.title('Sub-plot for 2 features')\n",
    "  plt.xlabel(feature_names[0])\n",
    "  plt.ylabel(feature_names[3])\n",
    "  \n",
    "\n",
    "# Let's do a scatter sub-plot of feature 1 vs. feature 2\n",
    "\n",
    "plt.figure(num=None, figsize=(4,4))\n",
    "plt.clf()\n",
    "for t in range(3):\n",
    "  if t == 0: \n",
    "       c = 'r'\n",
    "       marker = '>' \n",
    "  elif t == 1:\n",
    "       c = 'g'\n",
    "       marker = 'o' \n",
    "  elif t == 2:\n",
    "       c = 'b'\n",
    "       marker = 'x' \n",
    "    \n",
    "  plt.scatter(features[target == t,1], features[target == t,2], marker=marker, c=c)\n",
    "  plt.title('Sub-plot for 2 features')\n",
    "  plt.xlabel(feature_names[1])\n",
    "  plt.ylabel(feature_names[2])\n",
    "  \n",
    "\n",
    "# Let's do a scatter sub-plot of feature 1 vs. feature 3\n",
    "\n",
    "plt.figure(num=None, figsize=(4,4))\n",
    "plt.clf()\n",
    "for t in range(3):\n",
    "  if t == 0: \n",
    "       c = 'r'\n",
    "       marker = '>' \n",
    "  elif t == 1:\n",
    "       c = 'g'\n",
    "       marker = 'o' \n",
    "  elif t == 2:\n",
    "       c = 'b'\n",
    "       marker = 'x' \n",
    "    \n",
    "  plt.scatter(features[target == t,1], features[target == t,3], marker=marker, c=c)\n",
    "  plt.title('Sub-plot for 2 features')\n",
    "  plt.xlabel(feature_names[1])\n",
    "  plt.ylabel(feature_names[3])\n",
    "\n",
    "# Let's do a scatter sub-plot of feature 2 vs. feature 3\n",
    "\n",
    "plt.figure(num=None, figsize=(4,4))\n",
    "plt.clf()\n",
    "for t in range(3):\n",
    "  if t == 0: \n",
    "       c = 'r'\n",
    "       marker = '>' \n",
    "  elif t == 1:\n",
    "       c = 'g'\n",
    "       marker = 'o' \n",
    "  elif t == 2:\n",
    "       c = 'b'\n",
    "       marker = 'x' \n",
    "    \n",
    "  plt.scatter(features[target == t,2], features[target == t,3], marker=marker, c=c)\n",
    "  plt.title('Sub-plot for 2 features')\n",
    "  plt.xlabel(feature_names[2])\n",
    "  plt.ylabel(feature_names[3])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "By looking at the sub-plots,  **petal length** seems to be able to separate *Iris Setosa* from the other two flower species.  But how can we use Python program to help us to find this cutoff?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use NumPy indexing to get an array of strings:\n",
    "labels = target_names[target]  # labels is a list, each item is the label of the input\n",
    "\n",
    "# Extract feature 2, which is the petal length\n",
    "plength = features[:, 2]  # plength is a list with petal length of each input\n",
    "\n",
    "# build a boolean array (or tuple in Python)\n",
    "is_setosa = (labels == 'setosa')\n",
    "\n",
    "max_setosa = plength[is_setosa].max()   # find the maximun pedal length of setosa\n",
    "min_non_setosa = plength[~is_setosa].min() # find the minimum pedal length for non-setosa\n",
    "\n",
    "print('Maximum of setosa: {0}.'.format(max_setosa), ';  Minimum of others:{1}', format(min_non_setosa))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WE FOUND A CLASSIFIER for Setosa !!!!!\n",
    "\n",
    "**Classification rule**: If the petal length is smaller than 2, then this is an *Iris Setosa* flower; otherwise it is either *Iris Virginica* or *Versicolor*. \n",
    "\n",
    "Let's discover another classification rule to distinguish Virginica and Versicolor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best feature and its cut-off threshold so as to best distinguish Virginica from Versicolor\n",
    "#\n",
    "# The idea is this:  Let say we consider feature i and threshold t, \n",
    "# and those sample points whose feature i's values are less than t,\n",
    "# we say they are Virginica (Vericolor), while those sample points whose \n",
    "# feature i's values are >= t, we say are Vericolor (Virginica). \n",
    "#\n",
    "# For this feature i and threshold t, we compute the accuracy with the given labels.\n",
    "#\n",
    "# The threshold t can be found by sweeping ALL feature i's values.\n",
    "#\n",
    "# Therefore, for each feature i and threshold t, we have two configurations to consider:\n",
    "# Less than threshold t is Virginica (or Vericolor), while greater than or equal to t\n",
    "# is Vericolor (or Virginica).  This is the reason why we have the reverse in the code.\n",
    "# Finally, we have to consider for ALL 4 features and for all their values\n",
    "\n",
    "# First, let us select features and labels which are non-Setosa \n",
    "\n",
    "features = features[~is_setosa]   # now features array only contains 100 entries\n",
    "labels = labels[~is_setosa]       # now labels array only contains 100 entries\n",
    "\n",
    "\n",
    "is_virginica = (labels == 'virginica')   # create a boolean list to identify virginica\n",
    "\n",
    "\n",
    "#loop over all possible features and thresholds to see which one results in better accuracy\n",
    "\n",
    "\n",
    "best_acc = -1.0   # initialize best accuracy to be negative first\n",
    "\n",
    "for fi in range(features.shape[1]):  # loop through each 4 features\n",
    "   # test different thresholds\n",
    "   threshold = features[:,fi]   # list of potential thresholds for feature fi\n",
    "   for t in threshold:          # go through each threshold value\n",
    "     # access the vector for feature 'fi'\n",
    "      feature_i = features[:,fi]\n",
    "      # apply the threshold 't'\n",
    "      pred = (feature_i > t)    # build a boolean list of feature values to see if > t\n",
    "    \n",
    "      acc = (pred == is_virginica).mean()    # count average number of accuracy\n",
    "      rev_acc = (pred == ~is_virginica).mean()   # count average number of accuracy\n",
    "      if rev_acc > acc:\n",
    "            reverse = True\n",
    "            acc = rev_acc\n",
    "      else:\n",
    "            reverse = False\n",
    "      \n",
    "      if acc > best_acc:    # if acc is better than the current one, remember its state\n",
    "            best_acc = acc\n",
    "            best_fi = fi\n",
    "            best_t  = t\n",
    "            best_reverse = reverse\n",
    "            \n",
    "#  the variables best_fi, best_t, and best_reverse hold our model\n",
    "\n",
    "print('Best feature index is: ', best_fi)\n",
    "print('Best feature is: ', feature_names[best_fi])\n",
    "print('Best threshold is: ', best_t)\n",
    "print(\"Accuracy is: \", best_acc, '%')\n",
    "\n",
    "#print('best_fi:', best_fi, '; Best feature is: ', feature_names[best_fi], '; best_t:', best_t, '; best_acc:', best_acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remark:\n",
    "\n",
    "So far, we can accurately classify Setosa (with 100%) and find the *best feature* to classify  whether the flower is Virginica or Versicolor with 94% accuracy.  But this is good?\n",
    "\n",
    "Well, not really, this evaluation may be overly optimistic. We used the data to define what the threshold will be, and then we used the same data to evaluate the model. Of course, the model will perform better than anything else we tried on this dataset. The above reasoning is **circular**.\n",
    "\n",
    "What we  want to do is estimate the ability of the model to generalize to *new instances*. We should measure its performance in instances that the algorithm has **not seen at training**.\n",
    "\n",
    "This can be achieved via **cross-validation**.  In a nut shell, instead of using all data for training, we leave some for *validation*.  Let's illustrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simple illustration of cross-validation using \"leave-one-out\" for cross validation\n",
    "## In other words, just leave ONE data point for doing validation and the rest for training\n",
    "\n",
    "## Let's define some functions\n",
    "\n",
    "def is_virginica_test(fi, t, reverse, example):\n",
    "    'Apply threshold model to a new example to see whether it is correct or not.'\n",
    "    test = example[fi] > t\n",
    "    if reverse:\n",
    "        test = not test\n",
    "    return test\n",
    "\n",
    "#from threshold import fit_model, predict\n",
    "\n",
    "def fit_model(features, labels):\n",
    "    '''Learn a simple threshold model'''\n",
    "    best_acc = -1.0\n",
    "    # Loop over all the features:\n",
    "    for fi in range(features.shape[1]):\n",
    "        thresh = features[:, fi].copy()   # get all the feature values in fi\n",
    "        # test all feature values in order:\n",
    "        thresh.sort()     # sort first\n",
    "        for t in thresh:\n",
    "            pred = (features[:, fi] > t)\n",
    "\n",
    "            # Measure the accuracy of this \n",
    "            acc = (pred == labels).mean()\n",
    "\n",
    "            rev_acc = (pred == ~labels).mean()\n",
    "            if rev_acc > acc:\n",
    "                acc = rev_acc\n",
    "                reverse = True\n",
    "            else:\n",
    "                reverse = False\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                best_fi = fi\n",
    "                best_t = t\n",
    "                best_reverse = reverse\n",
    "\n",
    "    # A model is a threshold and an index\n",
    "    return best_t, best_fi, best_reverse\n",
    "\n",
    "def predict(model, features):\n",
    "    '''Apply a learned model'''\n",
    "    # A model is a pair as returned by fit_model\n",
    "    t, fi, reverse = model\n",
    "    if reverse:\n",
    "        return features[:, fi] <= t\n",
    "    else:\n",
    "        return features[:, fi] > t\n",
    "\n",
    "correct = 0.0     # initialize\n",
    "\n",
    "for ei in range(len(features)):   # go through all 100 sample points\n",
    "    # we will use all but the one at the position ei\n",
    "    training = np.ones(len(features), bool)  # make an array with 100 entries for True\n",
    "    training[ei] = False      # exclude the data item with index ei\n",
    "    testing = ~training\n",
    "    model = fit_model(features[training], is_virginica[training]) \n",
    "    predictions = predict(model, features[testing])\n",
    "    correct += np.sum(predictions == is_virginica[testing])\n",
    "\n",
    "acc = correct/float(len(features))\n",
    "print('Accuracy:{0:.1%}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remark\n",
    "\n",
    "For the above model, since we took out one sample before the training, so we break the circular argument !!!  This is also known as the **leave-one-out cross-validation**. \n",
    "\n",
    "We can *generalize* the above concept.  Instead of leaving one labeled sample point, we leave certain percentage of labeled sample points out. After training, we use those sample points for cross-validation.  This is called the **k-fold cross-validation**. \n",
    "\n",
    "The idea is to leave certain percentage of the sample points for cross-validation. Let say $k=5$ (e.g., separate the inputs into $k$ groups), it means we leave around $20$ (e.g., $\\frac{100}{k}$) of the data out for cross-validation. The idea is illustrated as follows:\n",
    "![image](figure-k-fold.png)\n",
    "\n",
    "For the above example, we have to go through training $5$ times, and the final accuracy is the **average** of the five experiments.  \n",
    "\n",
    "Note that when we have a large $k$, the computational complexity is higher but the accuracy is also higher (because we are using more input data for the training).  The rule of thumb is to use $5$-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore a classifier:  Decision Tree\n",
    "\n",
    "The above code assume we do *linear separation* along each feature.  In fact, we have something like this, and it is called **decision tree**, which can also handle separation with *multiple features*.\n",
    "\n",
    "In class, we will discuss the **decision tree** classifier/regressor.  Right now, just think\n",
    "of it as a way to divide-up the feature space so as to get a **good** \n",
    "(heuristcially speaking) accuracy result.  Let's study this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load decision tree classifier\n",
    "from sklearn import tree\n",
    "\n",
    "tr = tree.DecisionTreeClassifier(min_samples_leaf=10)\n",
    "\n",
    "# fit performs the learning (or it fits the model)\n",
    "tr.fit(features, labels)\n",
    "\n",
    "# Evaluating performance on the training set (which is not the right way because\n",
    "# we didn't split the data for training and testing !!!!!!! ) \n",
    "prediction = tr.predict(features)\n",
    "print(\"Accuracy of the decision tree: {:.1%}\".format(np.mean(prediction == labels)))\n",
    "\n",
    "# Plotting the decision tree (using an intermediate file):\n",
    "\n",
    "# You may need to first install graphviz via % pip install graphviz\n",
    "\n",
    "import graphviz\n",
    "\n",
    "tree.export_graphviz(tr, feature_names=feature_names, rounded=True, out_file='decision.dot')\n",
    "\n",
    "graphviz.Source(open('decision.dot').read())  # display the decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try to do a leave-one-out approach to check the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We remove entry i (i=0,1,2,3), for each entry we leave out, we train the model\n",
    "# Since we have four results, we average the accuracy\n",
    "\n",
    "predictions = []\n",
    "for i in range(len(features)):\n",
    "    train_features = np.delete(features, i, axis=0)  # train_features has 1 less \n",
    "    train_labels = np.delete(labels, i, axis=0)\n",
    "    tr.fit(train_features, train_labels)\n",
    "    predictions.append(tr.predict([features[i]]))\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "print(\"Accuracy (with LOO cross-validation):.{:.1%}\".\n",
    "              format(np.mean(predictions.ravel() == labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The right way to do leave-one-out in decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "predictions = model_selection.cross_val_predict(\n",
    "    tr,\n",
    "    features,\n",
    "    labels,\n",
    "    cv=model_selection.LeaveOneOut())\n",
    "print('Accuracy (with LOO cross-validation):',np.mean(predictions == labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A more complex dataset and a more advacned classifier\n",
    "\n",
    "Previously, we tried to find the *threshold* of in a feature to do classification. \n",
    "It is too simplistic because one can also find threshold for a **combination** of features.\n",
    "Let's example a more advanced classification algorithm and apply it on a more complicated\n",
    "dataset.\n",
    "\n",
    "The following argricultural dataset is too large to exhaustively plot out.  There are\n",
    "three possible wheat labels, they are\n",
    "* Canadians\n",
    "* Koma\n",
    "* Rosa\n",
    "\n",
    "There are seven features for each sample point, they are:\n",
    "1. Area $A$\n",
    "2. Perimeter $P$\n",
    "3. Compactness $C=4\\pi A/P^2$\n",
    "4. Length of kernel\n",
    "5. Width of kernel\n",
    "6. Asymmetry coefficient\n",
    "7. Length of kernel groove\n",
    "\n",
    "Notice that feature 3 derived from feature 1 and 2.  How to select which features, as well\n",
    "as how to *transform* these features, is called **feature engineering**.  In general,\n",
    "it is quite \"*laborious*\" but one can have better performance by appying simple algorithm on \n",
    "dataset with good features, than advanced algorithm on bad or poorly selected features.\n",
    "\n",
    "This dataset is in the directory:  *data/seeds.tsv*\n",
    "\n",
    "**We should take a look of that file.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest neighbor classifier as well as KNN classifier\n",
    "\n",
    "The nearest neighbor classifier is very simple. When classifying a new element, it looks at the training data for the object that is **closest** to it, its nearest neighbor. Then, it returns its label as the answer.\n",
    "\n",
    "One can also *generalize* the above classifer to the **KNN** model,that is, it looks \n",
    "not at a single neighbor, but to $k\\geq 1$ nearest ones and take a *majority vote* \n",
    "amongst the neighbors to do the classification.\n",
    "\n",
    "Scikit-learn has many built-in classification/regression models, including KNN.  Let's \n",
    "illustrate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###########################################\n",
    "############## SEEDS DATASET ##############\n",
    "###########################################\n",
    "\n",
    "from load import load_dataset\n",
    "\n",
    "# define feature_names\n",
    "\n",
    "feature_names = [\n",
    "    'area',\n",
    "    'perimeter',\n",
    "    'compactness',\n",
    "    'length of kernel',\n",
    "    'width of kernel',\n",
    "    'asymmetry coefficien',\n",
    "    'length of kernel groove',\n",
    "]\n",
    "\n",
    "features, labels = load_dataset('seeds')  # load both features and labels\n",
    "\n",
    "\n",
    "print('Shape of features: ', features.shape,'\\n')\n",
    "print('Shape of labels: ', labels.shape, '\\n')\n",
    "print('first 10 features are: ', features[:10],'\\n')\n",
    "print('first 10 labels are: ', labels[:10],'\\n')\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# define parameter for KNN\n",
    "k=1  # look for the NEAREST NEIGHBOR\n",
    "classifier = KNeighborsClassifier(n_neighbors=k)  # define  1NN\n",
    "\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=False)\n",
    "\n",
    "# now kf contains BOTH training and testing data points\n",
    "means = []\n",
    "for training, testing in kf.split(features):\n",
    "   # We learn a model for this fold with `fit` and then apply it to the\n",
    "   # testing data with `predict`:\n",
    "   classifier.fit(features[training], labels[training])  # do the training\n",
    "   prediction = classifier.predict(features[testing])    # do the cross-validation\n",
    "\n",
    "   # np.mean on an array of booleans returns fraction\n",
    "   # of correct decisions for this fold:\n",
    "   curmean = np.mean(prediction == labels[testing])  # compute the prediction rate\n",
    "   means.append(curmean)                             # append the prediction rate to means list\n",
    "    \n",
    "print('means list contains: ', means, '\\n')   # print out the means\n",
    "print('Mean accuracy: {:.1%}'.format(np.mean(means)))   # print out the aveage of means\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Normalizing features\n",
    "\n",
    "If you look at the above code, you will notice that different features have different ranges. This may create a bias in learning, in particular, when we have to use *distance* to \n",
    "find the *nearest* $k$ neighbors.\n",
    "\n",
    "In ML, we usually need to **normalize** all of the features to a common scale. This can\n",
    "be achieved using a *z-score*, which is a value reflecting\n",
    "how far away from the feature's mean, and in units of the feature's standard deviation. Mathematically, it is:\n",
    "$$f^{'} = \\frac{f-\\mu}{\\sigma}$$\n",
    "where $f$ is the original feature's value, $\\mu$ is the feature's average, and $\\sigma$ is\n",
    "feature's standard deviation.  Therefore $f^{'}$ will be *mean-centered* and *normalized*\n",
    "by the standard deviation.\n",
    "\n",
    "In scikit-learn, one can use *Pipeline* to achieve it.  Let's illustrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "classifier = KNeighborsClassifier(n_neighbors=1)    # call the 1NN classifier\n",
    "\n",
    "# The Pipeline constructor takes a list of pairs (str,clf). Each pair corresponds to a \n",
    "# step in the pipeline: the first element is a string naming the step, while the \n",
    "# second element is the object that performs the transformation.\n",
    "\n",
    "# Here, we use pipeline to chain two operations, one is the covert input into z-scores, then\n",
    "# pipe the input to the classifier defined above.  This becomes our ENHANCED classifier\n",
    "classifier = Pipeline([('norm', StandardScaler()), ('knn', classifier)])\n",
    "\n",
    "means = []\n",
    "for training,testing in kf.split(features):  # use the Kfold from previous cell\n",
    "    # We learn a model for this fold with `fit` and then apply it to the\n",
    "    # testing data with `predict`:\n",
    "    classifier.fit(features[training], labels[training])\n",
    "    prediction = classifier.predict(features[testing])\n",
    "\n",
    "    # np.mean on an array of booleans returns fraction\n",
    "    # of correct decisions for this fold:\n",
    "    curmean = np.mean(prediction == labels[testing])\n",
    "    means.append(curmean)\n",
    "print('Mean accuracy: {:.1%}'.format(np.mean(means)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the decision area for 1NN, 1NN with normalization, KNN where $k=11$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Note that this program may take around 30 seconds to run !!!!!\n",
    "\n",
    "COLOUR_FIGURE = True\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from load import load_dataset\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "feature_names = [\n",
    "    'area',\n",
    "    'perimeter',\n",
    "    'compactness',\n",
    "    'length of kernel',\n",
    "    'width of kernel',\n",
    "    'asymmetry coefficien',\n",
    "    'length of kernel groove',\n",
    "]\n",
    "\n",
    "\n",
    "def plot_decision(features, labels, num_neighbors=1):\n",
    "    '''Plots decision boundary for KNN\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    features : ndarray\n",
    "    labels : sequence\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fig : Matplotlib Figure\n",
    "    ax  : Matplotlib Axes\n",
    "    '''\n",
    "    y0, y1 = features[:, 2].min() * .9, features[:, 2].max() * 1.1\n",
    "    x0, x1 = features[:, 0].min() * .9, features[:, 0].max() * 1.1\n",
    "    X = np.linspace(x0, x1, 1000)\n",
    "    Y = np.linspace(y0, y1, 1000)\n",
    "    X, Y = np.meshgrid(X, Y)\n",
    "\n",
    "    model = KNeighborsClassifier(num_neighbors)\n",
    "    model.fit(features[:, (0,2)], labels)\n",
    "    C = model.predict(np.vstack([X.ravel(), Y.ravel()]).T).reshape(X.shape)\n",
    "    if COLOUR_FIGURE:\n",
    "        cmap = ListedColormap([(1., .7, .7), (.7, 1., .7), (.7, .7, 1.)])\n",
    "    else:\n",
    "        cmap = ListedColormap([(1., 1., 1.), (.2, .2, .2), (.6, .6, .6)])\n",
    "    fig,ax = plt.subplots()\n",
    "    ax.set_xlim(x0, x1)\n",
    "    ax.set_ylim(y0, y1)\n",
    "    ax.set_xlabel(feature_names[0])\n",
    "    ax.set_ylabel(feature_names[2])\n",
    "    ax.pcolormesh(X, Y, C, cmap=cmap)\n",
    "    if COLOUR_FIGURE:\n",
    "        cmap = ListedColormap([(1., .0, .0), (.1, .6, .1), (.0, .0, 1.)])\n",
    "        ax.scatter(features[:, 0], features[:, 2], c=labels, cmap=cmap)\n",
    "    else:\n",
    "        for lab, ma in zip(range(3), \"Do^\"):\n",
    "            ax.plot(features[labels == lab, 0], features[\n",
    "                     labels == lab, 2], ma, c=(1., 1., 1.), ms=6)\n",
    "    return fig,ax\n",
    "\n",
    "\n",
    "features, labels = load_dataset('seeds')\n",
    "names = sorted(set(labels))\n",
    "labels = np.array([names.index(ell) for ell in labels])\n",
    "\n",
    "fig,ax = plot_decision(features, labels)\n",
    "fig.tight_layout()\n",
    "fig.savefig('area_vs_compactness_1NN.png')\n",
    "\n",
    "features -= features.mean(0)\n",
    "features /= features.std(0)\n",
    "fig,ax = plot_decision(features, labels)\n",
    "fig.tight_layout()\n",
    "fig.savefig('area_vs_compactness_1NN_with_normalization.png')\n",
    "\n",
    "fig,ax = plot_decision(features, labels, 11)\n",
    "fig.tight_layout()\n",
    "fig.savefig('area_vs_compactness_11-NN_with_normalization.png')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forests\n",
    "\n",
    "There is an interesting paper,  [Do we Need Hundreds of Classifiers to Solve Real World Classification Problems?](http://jmlr.org/papers/v15/delgado14a.html) by Delgado et al. (2014) to understand why we recommend random forests as the **default classifier**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "rf = ensemble.RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# We use cross-validation to evaluate\n",
    "\n",
    "predict = model_selection.cross_val_predict(rf, features, labels)\n",
    "print(\"RF accuracy: {:.1%}\".format(np.mean(predict == labels)))\n",
    "\n",
    "# Let's plot it, we define a new decision function\n",
    "\n",
    "\n",
    "def plot_decision_space(clf, features, target, use_color=True):\n",
    "    from matplotlib.colors import ListedColormap\n",
    "\n",
    "    clf.fit(features[:, [0,2]], target)\n",
    "\n",
    "    y0, y1 = features[:, 2].min() * .9, features[:, 2].max() * 1.1\n",
    "    x0, x1 = features[:, 0].min() * .9, features[:, 0].max() * 1.1\n",
    "    X = np.linspace(x0, x1, 1000)\n",
    "    Y = np.linspace(y0, y1, 1000)\n",
    "    X, Y = np.meshgrid(X, Y)\n",
    "    C = clf.predict(np.vstack([X.ravel(), Y.ravel()]).T).reshape(X.shape)\n",
    "    if use_color:\n",
    "        cmap = ListedColormap([(1., .7, .7), (.7, 1., .7), (.7, .7, 1.)])\n",
    "    else:\n",
    "        cmap = ListedColormap([(1., 1., 1.), (.2, .2, .2), (.6, .6, .6)])\n",
    "\n",
    "    fig,ax = plt.subplots()\n",
    "    ax.scatter(features[:, 0], features[:, 2], c=target, cmap=cmap)\n",
    "    for lab, ma in zip(range(3), \"Do^\"):\n",
    "        ax.plot(features[target == lab, 0], features[\n",
    "                 target == lab, 2], ma, c=(1., 1., 1.), ms=6)\n",
    "\n",
    "    ax.set_xlim(x0, x1)\n",
    "    ax.set_ylim(y0, y1)\n",
    "    ax.set_xlabel(feature_names[0])\n",
    "    ax.set_ylabel(feature_names[2])\n",
    "    ax.pcolormesh(X, Y, C, cmap=cmap)\n",
    "    return fig\n",
    "\n",
    "_= plot_decision_space(rf, features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We have learnt: \n",
    "* Visualization subset of features in our dataset\n",
    "* From visualization, discover classification rules\n",
    "* Use of simple threshold technique to do classification\n",
    "* The need to split up the data into training and validation\n",
    "* From leave-one cross-validation to k-fold cross validation\n",
    "* Using 1NN and KNN as classifier\n",
    "* The need to **normalize** all features\n",
    "* Color scatter plot of results in KNN (with different values of $k$)\n",
    "* Classification via random forest\n",
    "\n",
    "<br>\n",
    "<span style=\"font-family:times; font-size:0.9em;\">\n",
    "p.s.: The above codes are the \"modified\" and \"enhanced\" version of the codes from the book, \"Building Machine Learning Systems with Python\".</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
