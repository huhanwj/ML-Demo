{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "duration": "1.5 hours"
   },
   "source": [
    "# Machine Learning with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Is scikit-learn?\n",
    "\n",
    "Scikit-learn provides a large range of algorithms in machine learning that are unified under a common and intuitive API. Most of the dozens of classes provided for various kinds of models share the large majority of the same calling interface. Very often—as we will see in examples below—you can easily substitute one algorithm for another with nearly no change in your underlying code. This allows you to explore the problem space quickly, and often arrive at an optimal, or at least satisficing$^1$ approach to your problem domain or datasets.\n",
    "\n",
    "* Simple and efficient tools for data mining and data analysis\n",
    "* Accessible to everybody, and reusable in various contexts\n",
    "* Built on NumPy, SciPy, and matplotlib\n",
    "* Open source, commercially usable - BSD license\n",
    "\n",
    "<hr/>\n",
    "\n",
    "<small>$^1$<i>Satisficing is a decision-making strategy of searching through the alternatives until an acceptability threshold is met. It is a portmanteau of satisfy and suffice, and was introduced by Herbert A. Simon in 1956. He maintained that many natural problems are characterized by computational intractability or a lack of information, both of which preclude the use of mathematical optimization procedures.</i></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Techniques Used in Machine Learning\n",
    "\n",
    "The diagram below is from the scikit-learn documentation, but the same general schematic of different techniques and algorithms that it outlines applies equally to any other library.  The classes represented in bubbles mostly will have equivalent versions in other libraries.\n",
    "\n",
    "\n",
    "\n",
    "![](img/sklearn-topics.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification versus Regression versus Clustering\n",
    "\n",
    "### Classification\n",
    "\n",
    "Classification is a type of **supervised learning** in which the targets for a prediction are a set of categorical values.\n",
    "\n",
    "### Regression\n",
    "\n",
    "Regression is a type of **supervised learning** in which the targets for a prediction are quantitative or continuous values.\n",
    "\n",
    "### Clustering\n",
    "\n",
    "Clustering is a type of **unsupervised learning** where you want to identify similarities among collections of items without an *a prior* classification scheme. You may or may not have an *a priori* about the number of categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical versus Ordinal versus Continuous Variables\n",
    "\n",
    "Features come in one of three basic types.\n",
    "\n",
    "### Categorical variables \n",
    "\n",
    "Some are **categorical** (also called nominal): A discrete set of values that a feature may assume, often named by words or codes (but sometimes confusingly as integers where an order may be misleadingly implied).\n",
    "\n",
    "### Ordinal variables\n",
    "\n",
    "Some are **ordinal**: There is a scale from low to high in the data values, but the spacing in the data may have little to no relationship to the underlying phenomenon. For example, while an airline or credit card \"reward program\" might have levels of Gold/Silver/Platinum/Diamond, there is probably no real sense in which Diamond is \"4 times as much\" as Gold, even though they are encoded as 1-4.\n",
    "\n",
    "### Continuous variables\n",
    "\n",
    "Some are **continuous** or quantitative: Some quantity is actually measured such that a number represents the amount of it. The distribution of these measurements is likely not to be uniform and linear (in which case scaling might be relevant), but there is a real thing being measured. Measurements might be quantized for continuous variables, but that does not necessarily make them ordinal instead. For example, we might measure annual rainfall in each town only to the nearest inch, and hence have integers for that feature.\n",
    "\n",
    "This notion of types of variables applies to statistics broadly. Some other concepts are genuinely specific to machine learning.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot Encoding\n",
    "\n",
    "For many machine learning algorithms, including neural networks, it is more useful to have a categorical feature with N possible values encoded as N features, each taking a binary value. Several tools, including a couple functions in scikit-learn will transform raw datasets into this format. Obviously, by encoding this way, dimensionality is increased.\n",
    "\n",
    "Let us illustrate using a toy test dataset.  The following whimsical data is suggested in a blog post by [Håkon Hapnes Strand](https://www.quora.com/What-is-one-hot-encoding-and-when-is-it-used-in-data-science).  Imagine we collected some data on individual organisms—namely taxonomic class, height, and lifespan.  Depending on our purpose, we might use this data for either supervised or unsupervised learning techniques (if we had a lot more observations, and a number more features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data: individual organism; height; lifespan\n",
    "\n",
    "data= [\n",
    "    ['human', 1.7, 85],\n",
    "    ['alien', 1.8, 92],\n",
    "    ['penguin', 1.2, 37],\n",
    "    ['octopus', 2.3, 25],\n",
    "    ['alien', 1.7, 85],\n",
    "    ['human', 1.2, 37],\n",
    "    ['octopus', 0.4, 8],\n",
    "    ['human', 2.0, 97]\n",
    "]\n",
    "\n",
    "print(data)  # perform a raw print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data with its original feature, just as a DataFrame\n",
    "import pandas as pd\n",
    "naive = pd.DataFrame(data, columns=['species', 'height (M)', 'lifespan (years)'])\n",
    "naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data one-hot encoded\n",
    "encoded = pd.get_dummies(naive)\n",
    "#encoded   # let's display before we replace those species' features\n",
    "\n",
    "encoded.columns = [c.replace('species_','') for c in encoded.columns]\n",
    "encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "y_true = [\"human\",   \"octopus\", \"human\", \"human\", \"octopus\", \"penguin\", \"penguin\"]\n",
    "y_pred = [\"octopus\", \"octopus\", \"human\", \"human\", \"octopus\", \"human\",   \"penguin\"]\n",
    "labels = ['octopus', 'penguin', 'human']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "#print('---------------')\n",
    "#print(cm[0,0])  # explore C_{0,1}\n",
    "#print('---------------')\n",
    "\n",
    "print(\"Confusion Matrix (predict/actual):\\n\", \n",
    "      pd.DataFrame(cm, index=labels, columns=labels), sep=\"  \")\n",
    "\n",
    "recall = np.diag(cm) / np.sum(cm, axis=1)\n",
    "print(\"\\nRecall:\\n\", pd.Series(recall, index=labels), sep=\"\")\n",
    "\n",
    "precision = np.diag(cm) / np.sum(cm, axis=0)\n",
    "print(\"\\nPrecision:\\n\", pd.Series(precision, index=labels), sep=\"\")\n",
    "\n",
    "print(\"\\nAccuracy:\\n\", np.sum(np.diag(cm)) / np.sum(cm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular case, F1 score is very close to accuracy.  In fact, using the \"micro\" averaging method reduces the result to accuracy.  Using the \"macro\" averaging makes it equivalent to a NumPy reduction from the formula given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "weighted_f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "print(\"\\nWeighted F1 score:\\n\", weighted_f1, sep=\"\")\n",
    "\n",
    "macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "print(\"\\nMacro F1 score:\\n\", macro_f1, sep=\"\")\n",
    "\n",
    "\n",
    "micro_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "print(\"\\nMicro F1 score:\\n\", micro_f1, sep=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Naive averaging F1 score:\", np.mean(2*(recall*precision)/(recall+precision)))\n",
    "print(\" sklearn macro averaging:\", f1_score(y_true, y_pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We have learnt: \n",
    "* Roadmap on scikit-learn: what learning models we want to use\n",
    "* Pandas and its dataframe\n",
    "* one-hot encoding\n",
    "* Confusion matrix\n",
    "* Accuracy\n",
    "* Precision\n",
    "* Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
