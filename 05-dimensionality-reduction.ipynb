{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lession 5: Dimensionality Reduction\n",
    "\n",
    "**Prepared by John C.S. Lui** \n",
    "\n",
    "Date: March 2nd, 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this lesssion, we learn about techniques of reducing the number of features.  \n",
    "\n",
    "Justifications for dimensionality reduction:\n",
    "1. There may be *useless features* that have no use in our classification/regression.\n",
    "2. Too many features may lead to a higher risk of overfitting.\n",
    "3. Reduce computational cost in training.\n",
    "4. Reduce the dimension to facility *visualization*\n",
    "\n",
    "Two broad classes of dimensionality redution:\n",
    "1. `Feature Selection`: Given $d$ features, want to reduce it to $k<d$ features\n",
    "    and still have high accuracy in ML tasks.\n",
    "2.  `Feature Projection`: Ususing statistical techniques to reduce number of features.\n",
    "     Example techniques we plan to cover are **Principle Component Analysis (PCA)**, \n",
    "     **multidimensional scaling (MDS)**, and **Linear Discriminant Analysis (LDA)**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "\n",
    "# We also need to define some functions which we will use\n",
    "\n",
    "import os\n",
    "CHART_DIR = \"charts\"  # define sub-directory to store charts (or figures)\n",
    "if not os.path.exists(CHART_DIR):\n",
    "    os.mkdir(CHART_DIR)  # create sub-directory\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')  # use ggplot, a statistical visualization package similar to R\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "DPI = 300  # define the dot-per-inch for plotting\n",
    "\n",
    "# define function to save file\n",
    "def save_png(name):\n",
    "    fn = 'Lesson_05_%s.png'%name \n",
    "    plt.savefig(os.path.join(CHART_DIR, fn), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "If two features $X_i$ and $X_j$ are highly correlated, then having both of them in the ML\n",
    "training may **NOT** help classification that much.   In statistics, we use **correlation** to\n",
    "quantify the *LINEAR RELATIONSHIP* between two features.\n",
    "\n",
    "One commone metric we often use is the **Pearson correlation coefficient** (also known\n",
    "as PCC), which has a value between +1 and −1, where 1 is total \n",
    "*positive* linear correlation, 0 is no linear correlation, and −1 is \n",
    "total *negative* linear correlation. PCC between random variables $X$ and $Y$ is\n",
    "$$\n",
    "   \\rho_{XY} = \\frac{E[(X-\\bar{X})(Y -\\bar{Y}]}{\\sigma_X \\sigma_Y}\n",
    "$$\n",
    "where $\\sigma_{Z}^{2} = E[(Z-\\bar{Z})^2]$, and $\\bar{Z}$ is the mean (or the first moment)\n",
    "of the random variable $Z$.  Scipy has utility to help us to compute the PCC of two features.\n",
    "\n",
    "In Scipy's pearsonr, for a given two equal-sized data series, it returns a **tuple** of \n",
    "two values, they are:\n",
    "* the correlation coefficient value, and\n",
    "* $p$-value:\n",
    "The $p$-value describes how likely it is that the data series has been generated by an uncorrelated system. In other words, the **higher** the $p$-value, \n",
    "the **less we should trust** the correlation coefficient.   Let's demonstrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr  # load pearsonr function\n",
    "\n",
    "My_X = [1,2,3,4,5,6,7,8,9,10]\n",
    "My_Y = [1,2,3,4,5,6,7,8,9,10.2]\n",
    "\n",
    "print('My_X and My_Y are correlated because the PPC tuple is: \\n', pearsonr(My_X, My_Y))\n",
    "\n",
    "Your_X = [1,2,3,4,5,6,7,8,9,10]\n",
    "Your_Y = [1, 20, 3,4, 5,6,7,8,9,10]\n",
    "print('Your_X and Your_Y are not correlated because the PPC tuple is: \\n', pearsonr(Your_X, Your_Y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's look at sample data\n",
    "\n",
    "We generate $x$ from 0 to 10 with increment of 0.2. For y, we let it be x times 0.5 and \n",
    "then offset it by a normal distribution with $\\mu=1$ and $\\sigma = [0.01, 0.1, 1.0, 10.0]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm  # import normal distribution generator\n",
    "\n",
    "def _plot_correlation_func(x, y):  # define function to generate plot for RVs x and your \n",
    "    r, p = pearsonr(x, y)\n",
    "    plt.scatter(x, y, c='r', s=15)\n",
    "    plt.title(\"Cor($X_1$, $X_2$) = %.3f, $r$= %.3f\" %(r, p))\n",
    "    plt.xlabel(\"$X_1$\")\n",
    "    plt.ylabel(\"$X_2$\")\n",
    "\n",
    "    f1 = scipy.poly1d(np.polyfit(x, y, 1))  # do a degree-1 polynomial fit\n",
    "    plt.plot(x, f1(x), \"b--\", linewidth=2);\n",
    "    \n",
    "np.random.seed(0)  # to reproduce the data later on\n",
    "plt.clf()\n",
    "plt.figure(num=None, figsize=(8, 8), dpi=DPI)\n",
    "\n",
    "x = np.arange(0, 10, 0.2)  # generate X from 0.0 to 10.0 with increment 0.2\n",
    "\n",
    "plt.subplot(221)\n",
    "y = 0.5 * x + norm.rvs(loc=1, scale=.01, size=len(x)) #loc is the mean, scale is the SD\n",
    "_plot_correlation_func(x, y)\n",
    "\n",
    "plt.subplot(222)\n",
    "y = 0.5 * x + norm.rvs(1, scale=.1, size=len(x))\n",
    "_plot_correlation_func(x, y)\n",
    "\n",
    "plt.subplot(223)\n",
    "y = 0.5 * x + norm.rvs(1, scale=1, size=len(x))\n",
    "_plot_correlation_func(x, y)\n",
    "\n",
    "plt.subplot(224)\n",
    "y = 0.5*x + norm.rvs(1, scale=10, size=len(x))\n",
    "_plot_correlation_func(x, y)\n",
    "\n",
    "plt.autoscale(tight=True)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "save_png(\"01_corr_demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation\n",
    "\n",
    "1.  For the first 3 plots, we can either drop $X_1$ or $X_2$\n",
    "2.  For the last plot, we need to keep *BOTH FEATURES*.\n",
    "\n",
    "**IMPORTANT NOTE**:  Notice that PPC only quanitify the **linear** correlation. When \n",
    "features are **non-linearly** correlated, we have a problem. Let's demonstrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(num=None, figsize=(8, 8), dpi=DPI)\n",
    "\n",
    "x = np.arange(-5, 5, 0.2) # generate X from -5.0 to 5.0 with increment of 0.2\n",
    "\n",
    "# Y = 0.5*X^2 plus some noise with is normally distributed\n",
    "plt.subplot(221)\n",
    "y = 0.5 * x ** 2 + norm.rvs(1, scale=.01, size=len(x))\n",
    "_plot_correlation_func(x, y)\n",
    "\n",
    "plt.subplot(222)\n",
    "y = 0.5 * x ** 2 + norm.rvs(1, scale=.1, size=len(x))\n",
    "_plot_correlation_func(x, y)\n",
    "\n",
    "plt.subplot(223)\n",
    "y = 0.5 * x ** 2 + norm.rvs(1, scale=1, size=len(x))\n",
    "_plot_correlation_func(x, y)\n",
    "\n",
    "plt.subplot(224)\n",
    "y = 0.5 * x ** 2 + norm.rvs(1, scale=10, size=len(x))\n",
    "_plot_correlation_func(x, y)\n",
    "\n",
    "plt.autoscale(tight=True)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "save_png(\"02_corr_demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation\n",
    "\n",
    "1.  Features $X_1$ and $X_2$ are correlated, but the Pearson r value is low.\n",
    "2.  If we do some *transformation*, say plot $X_1$ vs. $X_{1}^2$, then we can use PPC\n",
    "\n",
    "Can we pick a better measure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual Information\n",
    "\n",
    "Let's consider **mutual Information** via **entropy**.\n",
    "\n",
    "In simple terms, mutual information measures *how much information one feature ($X_2$)\n",
    "provides, given that we already have another feature (say $X_1$)*.  \n",
    "\n",
    "First, let us define the notion of *entropy* to \n",
    "measure the amount of information (or uncertainty) in a RV.  For a RV $X$, its entropy is:\n",
    "$$   H(X) = - \\sum_{\\forall x} \\mbox{Prob}\\,[X=x] \\log (\\mbox{Prob}\\,[X=x]) \n",
    "$$\n",
    "\n",
    "**Example:** Let say $X$ is a r.v. denoting a Bernoulli random variable of $x \\in \\{0,1\\}$\n",
    "and $\\mbox{Prob}[X=1]=p$ and $\\mbox{Prob}[X=0]=1-p$.  Using the above formula for\n",
    "entropy, when $p=0.5$, $H(X)=1.0$. In fact, The entropy has always between 0 to 1.\n",
    "Let's illustrate $H(X)$ for different values of $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "plt.clf()\n",
    "plt.figure(num=None, figsize=(4, 4), dpi=DPI)\n",
    "\n",
    "plt.title(\"Entropy $H(X)$\")\n",
    "plt.xlabel(\"$P(X=1)=p$\")\n",
    "plt.ylabel(\"$H(X)$\")\n",
    "\n",
    "plt.xlim(xmin=0, xmax=1.1)\n",
    "x = np.arange(0.001, 1, 0.001)  # generate different value of $p$\n",
    "y = -x * np.log2(x) - (1 - x) * np.log2(1 - x)\n",
    "plt.plot(x, y, c='black')\n",
    "\n",
    "plt.autoscale(tight=True)\n",
    "plt.grid(True)\n",
    "plt.ylim((0,1.05))\n",
    "plt.xlim((-.01,1.01))\n",
    "\n",
    "save_png('03_entropy_demo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual Information\n",
    "\n",
    "Mutual information between RVs $X$ and $Y$ is\n",
    "$$\n",
    "I(X;Y) = \\sum_{\\forall x} \\sum_{\\forall y} P(X=x, Y=y) \\log \\frac{P(X=x,Y=y)}{P(X=x)P(Y=y)}\n",
    "$$\n",
    "\n",
    "Often times, we want to work with the **normalized mutual information**, $N\\!I(X;Y)$, \n",
    "which is\n",
    "$$  N\\!I(X;Y) = \\frac{I(X;Y)}{H(X) + H(Y)}\n",
    "$$\n",
    "\n",
    "The implementation of the mutual information we can can be done by **binning** \n",
    "the feature values, and then calculating the fraction of values in each bin.   Let's illustrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define function to calculate mutual information\n",
    "def normalized_mutual_info(x, y, bins=10):\n",
    "    counts_xy, bins_x, bins_y = np.histogram2d(x, y, bins=(bins, bins))\n",
    "    counts_x, bins = np.histogram(x, bins=bins)\n",
    "    counts_y, bins = np.histogram(y, bins=bins)\n",
    "\n",
    "    counts_xy += 1\n",
    "    counts_x += 1\n",
    "    counts_y += 1\n",
    "    P_xy = counts_xy / np.sum(counts_xy)\n",
    "    P_x = counts_x / np.sum(counts_x)\n",
    "    P_y = counts_y / np.sum(counts_y)\n",
    "\n",
    "    I_xy = np.sum(P_xy * np.log2(P_xy / (P_x.reshape(-1, 1) * P_y)))\n",
    "\n",
    "    return I_xy / (entropy(counts_x) + entropy(counts_y))\n",
    "\n",
    "def _plot_mi_func(x, y):\n",
    "    mi = normalized_mutual_info(x, y)\n",
    "    plt.scatter(x, y, s=15)\n",
    "    plt.title(\"NI($X_1$, $X_2$) = %.3f\" % mi)\n",
    "    plt.xlabel(\"$X_1$\")\n",
    "    plt.ylabel(\"$X_2$\")\n",
    "    \n",
    "\n",
    "np.random.seed(0)  # to reproduce the data later on\n",
    "plt.clf()\n",
    "plt.figure(num=None, figsize=(8, 8), dpi=DPI)\n",
    "\n",
    "x = np.arange(0, 10, 0.2)\n",
    "\n",
    "plt.subplot(221)\n",
    "y = 0.5 * x + norm.rvs(1, scale=.01, size=len(x))\n",
    "_plot_mi_func(x, y)\n",
    "\n",
    "plt.subplot(222)\n",
    "y = 0.5 * x + norm.rvs(1, scale=.1, size=len(x))\n",
    "_plot_mi_func(x, y)\n",
    "\n",
    "plt.subplot(223)\n",
    "y = 0.5 * x + norm.rvs(1, scale=1, size=len(x))\n",
    "_plot_mi_func(x, y)\n",
    "\n",
    "plt.subplot(224)\n",
    "y = 0.5*x + norm.rvs(1, scale=10, size=len(x))\n",
    "_plot_mi_func(x, y)\n",
    "\n",
    "plt.autoscale(tight=True)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "save_png('04_mi_demo_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So mutual information can help us to discover *linear relationship*.  How about *non-linear*?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.figure(num=None, figsize=(8, 8), dpi=DPI)\n",
    "\n",
    "x = np.arange(-5, 5, 0.2) # generate x from -5 to 5 with increment of 0.2\n",
    "\n",
    "# y = 0.5 * x^2 + noise, where noise is normally distributed\n",
    "plt.subplot(221)\n",
    "y = 0.5 * x ** 2 + norm.rvs(1, scale=.01, size=len(x))\n",
    "_plot_mi_func(x, y)\n",
    "\n",
    "plt.subplot(222)\n",
    "y = 0.5 * x ** 2 + norm.rvs(1, scale=.1, size=len(x))\n",
    "_plot_mi_func(x, y)\n",
    "\n",
    "plt.subplot(223)\n",
    "y = 0.5 * x ** 2 + norm.rvs(1, scale=1, size=len(x))\n",
    "_plot_mi_func(x, y)\n",
    "\n",
    "plt.subplot(224)\n",
    "y = 0.5 * x ** 2 + norm.rvs(1, scale=10, size=len(x))\n",
    "_plot_mi_func(x, y)\n",
    "\n",
    "plt.autoscale(tight=True)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "save_png('05_mi_demo_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation\n",
    "\n",
    "So mutual information can be used to reveal **BOTH** *linear* and *non-linear* relationship between two features\n",
    "\n",
    "So we can do the following:\n",
    "1. Calculate the normalized mutual information for **all feature pairs**. For every\n",
    "   pair with too high a value, which we would have to pre-determine this threshold, we \n",
    "   would then drop one of these two features.\n",
    "2. For regression, we could drop the feature that has **too little mutual information** \n",
    "   with the targeted value.\n",
    "   \n",
    "This is ok for small number of features. If $d=1,000,000$, then we need to consider\n",
    "$1000000 \\times 999999$ pairs. Luckily, we can use various **feature projection methods**, like the **principle component analysis (PCA)**.\n",
    "\n",
    "Another disadvantage is that we discard features that may not be that important in\n",
    "isolation, but at times, we can find that some *combination of inputs* may determine \n",
    "the output. A good example is the XOR function wherein $X_1$ and $X_2$ are two input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ML models to do feature selection (or filtering)\n",
    "\n",
    "Another way people often use is to rely on the underlying ML model to do feature selection\n",
    "via *filtering*.  This is known as **wrapper** and it can be achieved via the following:\n",
    "\n",
    "1. Use features in X\n",
    "2. Using model Y and input features in X to do the training\n",
    "3. After step 2, examine the *importance* of individual feature in X\n",
    "4. If the number of features is too big, then *filter* some least important ones from X, go to step 1.  Else, terminate.\n",
    "\n",
    "In scikit-learn, there are various excellent wrapper classes in the `sklearn.feature_selection` package. A real workhorse in this field is\n",
    "the *recursive feature elimination* (or `RFE`). It takes an estimator and the \n",
    "desired number of features to keep as parameters,  then trains the estimator with \n",
    "various feature sets. Let's illustrate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.datasets import make_classification  # make_classification is a data generator\n",
    "\n",
    "# we generate 100 samples, each with 10 features, informative feature is only 3\n",
    "X, y = make_classification(n_samples=100, n_features=10, n_informative=3, random_state=0)\n",
    "\n",
    "#print('X=', X)\n",
    "#print('y=', y)\n",
    "\n",
    "#print('Type of X=', type(X), '; dimension of X=', X.shape)\n",
    "#print('Type of y=', type(y), '; dimension of y=', y.shape)\n",
    "\n",
    "clf = LogisticRegression()    # create handle for LR\n",
    "clf.fit(X, y)\n",
    "\n",
    "selector = RFE(clf, n_features_to_select=3)  # call REF and state we need to select the top 3 features\n",
    "selector = selector.fit(X, y)   # fit model to REF selector\n",
    "print(selector.support_)    # print which feature to select\n",
    "print(selector.ranking_)    # print ranking of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's loop through to select 1,2,...,10 features to select, and see which features wil\n",
    "# be selected\n",
    "\n",
    "for i in range(1, 11):\n",
    "    selector = RFE(clf, n_features_to_select=i)\n",
    "    selector = selector.fit(X, y)\n",
    "    print(\"i=%i\\t support=%s\\n\\t ranking=%s\" % (i, selector.support_, selector.ranking_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation\n",
    "\n",
    "1. When we use RFE to select features, the results are *stable*: features which are kept\n",
    "   when we set a lower threshold are also included when we increase the threshold\n",
    "   \n",
    "**Note**: During model training, we also use train/test set splitting to warn us \n",
    "when we are not doing correctly (e.g., low classification training/testing performance).\n",
    "Also, many learners have the built-in feature selection (e.g., decision tree). Also,\n",
    "learners can use regularization, say $L_1$ norm, to filter those features that are not\n",
    "that important.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Projection Methods\n",
    "\n",
    "At times, even when we apply feature selection, we still find the feature space is too\n",
    "large, or when we need to reduce a huge feature space to 2 or 3 so to *visualize* it. We\n",
    "often use the feature projection methods.  Let's go through some of them here:\n",
    "1. Principal component analysis (**PCA**), which is an *unsupervised linear projection* method\n",
    "2. Linear Discriminant Analysis (**LDA**), which is a *supervised linear projection* method\n",
    "3. Multidimensional scaling (**MDS**), a *non-linear projection* method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Compnent Analysis (PCA)\n",
    "\n",
    "Given the input $X$, PCA finds a linear projection of $X$ in a lower dimensional space \n",
    "such that it has the following properties:\n",
    "1. The resulting variance is maximized\n",
    "2. The final reconstruction error is minimized\n",
    "\n",
    "PCA can be applied to classification or regressin problems, its basic algorithm is:\n",
    "1. Given $X$, make sure all features are centered (by subtracting the feature's average)\n",
    "2. Compute the covariance matrix\n",
    "3. Compute the eigenvectors and eigenvalues of the covariance matrix\n",
    "\n",
    "To demonstrate, assume that we start with $d = 1000$ features and our learner can\n",
    "only handle no more than 20 features. So we pick the 20 eigenvectors with the **highest eigenvalues**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition # get PCA from deomposition package\n",
    "\n",
    "np.random.seed(3)  # pin down the random seed\n",
    "\n",
    "#x1 = np.arange(0, 10, .2)  # generate x from 0 to 10 with increment of 0.2\n",
    "#x2 = x1 + np.random.normal(scale=1, size=len(x1)) # x2 is like x1 with added noise\n",
    "\n",
    "plt.clf()\n",
    "fig = plt.figure(num=None, figsize=(10, 4), dpi=DPI)\n",
    "plt.subplot(121)\n",
    "\n",
    "plt.title(\"Original feature space\")\n",
    "plt.xlabel(\"$X_1$\")\n",
    "plt.ylabel(\"$X_2$\")\n",
    "\n",
    "x1 = np.arange(0, 10, .2)  # generate x from 0 to 10 with increment of 0.2\n",
    "x2 = x1 + np.random.normal(scale=1, size=len(x1))  # x2 is like x1 with added noise\n",
    "\n",
    "good = (x1 > 5) | (x2 > 5) # define array of good points (or class 1 points)\n",
    "bad = ~good                # define array of bad points (or class 0 points)\n",
    "\n",
    "plt.scatter(x1[good], x2[good], edgecolor=\"blue\", facecolor=\"blue\", s=15)\n",
    "plt.scatter(x1[bad], x2[bad], edgecolor=\"red\", facecolor=\"white\", s=15)\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(122)\n",
    "\n",
    "X = np.c_[(x1, x2)]\n",
    "\n",
    "# setting parameters for PCA, where n_components is the # of features we want to reduce to\n",
    "pca = decomposition.PCA(n_components=1)   # from d=2 to k=1\n",
    "Xtrans = pca.fit_transform(X)\n",
    "\n",
    "Xg = Xtrans[good]\n",
    "Xb = Xtrans[bad]\n",
    "\n",
    "plt.scatter(Xg[:, 0], np.zeros(len(Xg)), edgecolor=\"blue\", facecolor=\"blue\", s=15)\n",
    "plt.scatter(Xb[:, 0], np.zeros(len(Xb)), edgecolor=\"red\", facecolor=\"white\", s=15)\n",
    "plt.title(\"Transformed feature space\")\n",
    "plt.xlabel(\"$X'$\")\n",
    "fig.axes[1].get_yaxis().set_visible(False)\n",
    "\n",
    "#print out the variance ratio of the first eigenvector\n",
    "print('The variance ratio of the 1st eigenvector = ', pca.explained_variance_ratio_) \n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.autoscale(tight=True)\n",
    "save_png(\"06_pca_demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to determine the number of dimension $k$ in scikit-learn?\n",
    "\n",
    "Usually, we do not know what number of dimensions ($k$) is good. In that case, we leave the `n_components` parameter unspecified when initializing PCA, it will then calculate the \n",
    "**full transformation** (up to the original dimension $d$). After fitting the data,\n",
    "`explained_variance_ratio_` contains an array of ratios in *decreasing order*: the \n",
    "first value is the ratio of the basis vector describing the direction of the *highest* variance, the second value is the ratio of the direction of the *second highest* variance,\n",
    "and so on. \n",
    "\n",
    "Plots displaying the explained variance over the number of components are called \n",
    "**scree plots**. A nice example of combining a scree plot with a grid search to find the best setting for the classification problem can be found at http://scikit-learn.org/stable/auto_examples/plot_digits_pipe.html.\n",
    "\n",
    "**Note**: In scikit-learn, there is a module `Kernel PCA`, which can do dimensionality\n",
    "reduction for *non-linear project*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitation of PCA\n",
    "\n",
    "Note that PCA does not use the label information to do dimensionality reduction.  So some times, PCA does the dimensionality reduction job, but it may make classification problem\n",
    "a lot more difficult.  Let's illustrate this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "fig = plt.figure(num=None, figsize=(10, 4), dpi=DPI)\n",
    "plt.subplot(121)\n",
    "\n",
    "plt.title(\"Original feature space\\n (Class 1 when $X_1 > X_2$)\")\n",
    "plt.xlabel(\"$X_1$\")\n",
    "plt.ylabel(\"$X_2$\")\n",
    "\n",
    "x1 = np.arange(0, 10, .2)\n",
    "x2 = x1 + np.random.normal(scale=1, size=len(x1))\n",
    "\n",
    "good = x1 > x2  # define what are class 1 points\n",
    "bad = ~good     # complement is class 2 points\n",
    "\n",
    "plt.scatter(x1[good], x2[good], edgecolor=\"blue\", facecolor=\"blue\", s=15)\n",
    "plt.scatter(x1[bad], x2[bad], edgecolor=\"red\", facecolor=\"white\", s=15)\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(122)  # prepare the 2nd plot\n",
    "\n",
    "X = np.c_[(x1, x2)]\n",
    "\n",
    "pca = decomposition.PCA(n_components=1)  # reduce it to 1 feature\n",
    "Xtrans = pca.fit_transform(X)\n",
    "\n",
    "Xg = Xtrans[good]\n",
    "Xb = Xtrans[bad]\n",
    "\n",
    "plt.scatter(Xg[:, 0], np.zeros(len(Xg)), edgecolor=\"blue\", facecolor=\"blue\", s=15)\n",
    "plt.scatter(Xb[:, 0], np.zeros(len(Xb)), edgecolor=\"red\", facecolor=\"white\", s=15)\n",
    "plt.title(\"Transformed feature space\")\n",
    "plt.xlabel(\"$X'$\")\n",
    "fig.axes[1].get_yaxis().set_visible(False) # turn off y-axis\n",
    "\n",
    "print('The variance ratio of the 1st eigenvector = ', pca.explained_variance_ratio_) \n",
    "\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.autoscale(tight=True)\n",
    "save_png(\"07_pca_demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation\n",
    "\n",
    "Note that class 1 points are inter-mixed with class 0 points.  It becomes more difficult\n",
    "to do classification !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis (LDA)\n",
    "\n",
    "LDA is a linear supervised dimensionality reduction technique.  Unlike PCA, LDA uses\n",
    "the label information.  Its objective is to do projection so that it can \n",
    "maximize the distance of points belonging to different classes, while minimizing the distances of points of the same class.  This way, after the projection, \n",
    "classification can be easily made.  Let's illustrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the LDA module\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "plt.clf()\n",
    "fig = plt.figure(num=None, figsize=(10, 4), dpi=DPI)\n",
    "plt.subplot(121)\n",
    "\n",
    "plt.title(\"Original feature space\\n (Class 1 when $X_1 > X_2$)\")\n",
    "plt.xlabel(\"$X_1$\")\n",
    "plt.ylabel(\"$X_2$\")\n",
    "\n",
    "good = x1 > x2\n",
    "bad = ~good\n",
    "\n",
    "plt.scatter(x1[good], x2[good], edgecolor=\"blue\", facecolor=\"blue\", s=15)\n",
    "plt.scatter(x1[bad], x2[bad], edgecolor=\"red\", facecolor=\"white\", s=15)\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(122)\n",
    "\n",
    "X = np.c_[(x1, x2)]\n",
    "\n",
    "lda_inst = LinearDiscriminantAnalysis(n_components=1) # instruct LDA to reduce to 1 dimension\n",
    "Xtrans = lda_inst.fit_transform(X, good)  # we also provide labels to the fit_transform !!!\n",
    "\n",
    "Xg = Xtrans[good]\n",
    "Xb = Xtrans[bad]\n",
    "\n",
    "plt.scatter(Xg[:, 0], np.zeros(len(Xg)), edgecolor=\"blue\", facecolor=\"blue\", s=15)\n",
    "plt.scatter(Xb[:, 0], np.zeros(len(Xb)), edgecolor=\"red\", facecolor=\"white\", s=15)\n",
    "plt.title(\"Transformed feature space\")\n",
    "plt.xlabel(\"$X'$\")\n",
    "fig.axes[1].get_yaxis().set_visible(False)\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.autoscale(tight=True)\n",
    "save_png(\"08_lda_demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multidimensional Scaling (MDS)\n",
    "\n",
    "MDS is a non-linear projection method, and its aim is to retain the \n",
    "*relative distances* among points as much as possible when reducing the dimensions. This \n",
    "is especially useful if we wan to *visualize* the data points at lower dimension. It applies\n",
    "a distance function $f_d()$ (say Euclidean distance), and applies it as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\left( \\begin{array}{ccc}\n",
    "         x_{11} & \\cdots & x_{1d} \\\\\n",
    "         \\vdots & \\vdots & \\vdots \\\\\n",
    "         x_{N1} & \\cdots & x_{Nd}\n",
    "       \\end{array}\n",
    "\\right)   \\rightarrow \n",
    "\\left( \\begin{array}{ccc}\n",
    "         f_d(X_1, X_1) & \\cdots & f_d(X_1, X_N) \\\\\n",
    "         \\vdots & \\vdots & \\vdots \\\\\n",
    "         f_d(X_N, X_1) & \\cdots & f_d(X_N, X_N)\n",
    "       \\end{array}\n",
    "\\right)\n",
    "\\end{equation*}\n",
    "\n",
    "Given this distance between any pair of points, MDS tries to position the individual \n",
    "data points in the lower dimensional space (usually two or three) such that the\n",
    "new distance  resembles the distances in the original space as much as possible.\n",
    "Let's illustrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's consider 3 points in 5-dimensional space\n",
    "X = np.c_[np.ones(5), 2 * np.ones(5), 10 * np.ones(5)].T  # remember the transpose\n",
    "print(X)\n",
    "\n",
    "np.array([0,1,2,3,4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import manifold, decomposition, datasets\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "np.random.seed(3) # for renegeration\n",
    "\n",
    "# all examples will have three classes in this file\n",
    "colors = ['g', 'r', 'b', 'k','c']\n",
    "markers = ['o', 'P', 'X', 'D','s']\n",
    "\n",
    "# create five points in which each point is 5-dimensional\n",
    "X = np.c_[np.ones(5), 2 * np.ones(5), np.array([0,3,2,2,4]), \n",
    "          np.array([7,4,5,6,4]), 10 * np.ones(5)].T\n",
    "y = np.array([0, 1, 2, 3, 4])\n",
    "\n",
    "fig = plt.figure(figsize=(10, 4), dpi=DPI)\n",
    "\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ax.set_facecolor('lightgrey')\n",
    "\n",
    "mds = manifold.MDS(n_components=3)  # MDS and ask to reduce to 3 features\n",
    "Xtrans = mds.fit_transform(X)\n",
    "\n",
    "for cl, color, marker in zip(np.unique(y), colors, markers):\n",
    "    ax.scatter(\n",
    "        Xtrans[y == cl][:, 0], Xtrans[y == cl][:, 1], Xtrans[y == cl][:, 2], c=color, marker=marker, edgecolor='black')\n",
    "plt.title(\"MDS on example data set\\n in 3 dimensions\\n\")\n",
    "ax.view_init(10, -15)\n",
    "\n",
    "mds = manifold.MDS(n_components=2)\n",
    "Xtrans = mds.fit_transform(X)\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "for cl, color, marker in zip(np.unique(y), colors, markers):\n",
    "    ax.scatter(\n",
    "        Xtrans[y == cl][:, 0], Xtrans[y == cl][:, 1], c=color, marker=marker, edgecolor='lightgrey')\n",
    "plt.title(\"MDS on example data set\\n in 2 dimensions\")\n",
    "\n",
    "save_png(\"09_mds_demo_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDS Visualization of the Iris dataset\n",
    "\n",
    "Let's consider the Iris dataset which contains four attributes per data point. We will\n",
    "project it into three-dimensional space while keeping the relative distances between \n",
    "the individual flowers as much as possible. If we do not specify any metric, so MDS will default to Euclidean.  Let's proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# MDS\n",
    "\n",
    "fig = plt.figure(figsize=(10, 4), dpi=DPI)\n",
    "\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ax.set_facecolor('lightgrey')\n",
    "\n",
    "mds = manifold.MDS(n_components=3)\n",
    "Xtrans = mds.fit_transform(X)\n",
    "\n",
    "for cl, color, marker in zip(np.unique(y), colors, markers):\n",
    "    ax.scatter(\n",
    "        Xtrans[y == cl][:, 0], Xtrans[y == cl][:, 1], Xtrans[y == cl][:, 2], c=color, marker=marker, edgecolor='black')\n",
    "plt.title(\"MDS on Iris data set\\n in 3 dimensions\\n\")\n",
    "ax.view_init(10, -15)\n",
    "\n",
    "mds = manifold.MDS(n_components=2)\n",
    "Xtrans = mds.fit_transform(X)\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "for cl, color, marker in zip(np.unique(y), colors, markers):\n",
    "    ax.scatter(\n",
    "        Xtrans[y == cl][:, 0], Xtrans[y == cl][:, 1], c=color, marker=marker, edgecolor='black')\n",
    "plt.title(\"MDS on Iris data set in 2 dimensions\")\n",
    "\n",
    "save_png(\"10_mds_demo_iris.png\")\n",
    "\n",
    "# PCA\n",
    "\n",
    "fig = plt.figure(figsize=(10, 4), dpi=DPI)\n",
    "\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ax.set_facecolor('lightgrey')\n",
    "\n",
    "pca = decomposition.PCA(n_components=3)\n",
    "Xtrans = pca.fit(X).transform(X)\n",
    "\n",
    "for cl, color, marker in zip(np.unique(y), colors, markers):\n",
    "    ax.scatter(\n",
    "        Xtrans[y == cl][:, 0], Xtrans[y == cl][:, 1], Xtrans[y == cl][:, 2], c=color, marker=marker, edgecolor='black')\n",
    "plt.title(\"PCA on Iris data set \\nin 3 dimensions\\n\")\n",
    "ax.view_init(50, -35)\n",
    "\n",
    "pca = decomposition.PCA(n_components=2)\n",
    "Xtrans = pca.fit_transform(X)\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "for cl, color, marker in zip(np.unique(y), colors, markers):\n",
    "    ax.scatter(Xtrans[y == cl][:, 0], Xtrans[y == cl][:, 1], c=color, marker=marker, edgecolor='black')\n",
    "plt.title(\"PCA on Iris data set in 2 dimensions\")\n",
    "plt.tight_layout()\n",
    "\n",
    "save_png(\"11_pca_demo_iris\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation and remark\n",
    "1. For PCA, it has a larger expected  spread of flowers belonging to the same class.\n",
    "2. MDS, by deault, use Euclidean distance function, but you can define your own.\n",
    "3. For categorical data, one *cannot* use Euclidean distance, so you have to devicee\n",
    "   your own distance function.\n",
    "4. Both MDS and PCA are not a single algorithm, but rather a **family of different algorithms**. Please refer to the documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We have discussed:\n",
    "1. Feature selection vs. feature projection methods\n",
    "2. How to use correlation, in particular, Pearson Coefficient, to find out linear \n",
    "   relationship among two features\n",
    "3. Discuss how to use mutual information to discover linear and non-linear relations between two features.\n",
    "4. Discuss how to use *recursive wrapper* to select features.\n",
    "5. Discuss PCA, LDA and Multidimensioal Scaling (MDS).\n",
    "6. There are other dimensionality reduction techniques, for example,\n",
    "   **Isomap**, a variant of MDS, which tries to match distances between data but instead\n",
    "   of Euclidean distance, it uses geodesic distances.  Another one is \n",
    "   **Laplacian eigenmaps**, which tries to match similarities between data,\n",
    "\n",
    "\n",
    "<br>\n",
    "<span style=\"font-family:times; font-size:0.9em;\">\n",
    "p.s.: The above codes are the \"modified\" and \"enhanced\" version of the codes from the book, \"Building Machine Learning Systems with Python\".</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
